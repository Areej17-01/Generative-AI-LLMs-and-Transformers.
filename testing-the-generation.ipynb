{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:00:32.377515Z","iopub.execute_input":"2025-02-04T08:00:32.377835Z","iopub.status.idle":"2025-02-04T08:00:37.684498Z","shell.execute_reply.started":"2025-02-04T08:00:32.377808Z","shell.execute_reply":"2025-02-04T08:00:37.683622Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:01:29.428226Z","iopub.execute_input":"2025-02-04T08:01:29.428506Z","iopub.status.idle":"2025-02-04T08:01:34.335884Z","shell.execute_reply.started":"2025-02-04T08:01:29.428484Z","shell.execute_reply":"2025-02-04T08:01:34.335255Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = \"cuda\"\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:01:54.850806Z","iopub.execute_input":"2025-02-04T08:01:54.851256Z","iopub.status.idle":"2025-02-04T08:01:54.854725Z","shell.execute_reply.started":"2025-02-04T08:01:54.851231Z","shell.execute_reply":"2025-02-04T08:01:54.854091Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.config.max_position_embeddings\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:01:57.829952Z","iopub.execute_input":"2025-02-04T08:01:57.830260Z","iopub.status.idle":"2025-02-04T08:03:11.392887Z","shell.execute_reply.started":"2025-02-04T08:01:57.830237Z","shell.execute_reply":"2025-02-04T08:03:11.392229Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f4dc2508b1e430dbaf6629169358141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b1a77236654fe3a2a4158928a494c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a88939a6ef443a7b6a2f4ec5865ac01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e94c023837947e796b9a37e625f5982"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d19091e81a1480a967fc96d8d144e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f55e17c0a7d947eab4c9c95a988c1ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e80f77f0520842cf8f99d5765737f256"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"messages = [\n   {\"role\": \"user\", \"content\": \"How do chat templates work?\"},\n   {\"role\": \"assistant\", \"content\": \"Chat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.\"},\n   {\"role\": \"user\", \"content\": \"How do I use them?\"},\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:13:04.737223Z","iopub.execute_input":"2025-02-04T08:13:04.737885Z","iopub.status.idle":"2025-02-04T08:13:04.741750Z","shell.execute_reply.started":"2025-02-04T08:13:04.737857Z","shell.execute_reply":"2025-02-04T08:13:04.741031Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"encoded = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\nencoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:13:13.961172Z","iopub.execute_input":"2025-02-04T08:13:13.961458Z","iopub.status.idle":"2025-02-04T08:13:14.008406Z","shell.execute_reply.started":"2025-02-04T08:13:13.961438Z","shell.execute_reply":"2025-02-04T08:13:14.007713Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor([[  529, 29989,  1792, 29989, 29958,    13,  5328,   437, 13563, 17475,\n           664, 29973,     2, 29871,    13, 29966, 29989,   465, 22137, 29989,\n         29958,    13,  1451,   271, 17475,  1371, 29871,   365, 26369, 29879,\n           763,   592,  5706,   901, 16165,   261,   296, 20890,   491, 13138,\n           263,  2281,  2955,   982,   304,  2894,   675,   278, 14983, 29889,\n             2, 29871,    13, 29966, 29989,  1792, 29989, 29958,    13,  5328,\n           437,   306,   671,   963, 29973,     2, 29871,    13, 29966, 29989,\n           465, 22137, 29989, 29958,    13]])"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"model_inputs = encoded.to(device)\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:13:26.434501Z","iopub.execute_input":"2025-02-04T08:13:26.434835Z","iopub.status.idle":"2025-02-04T08:13:28.120072Z","shell.execute_reply.started":"2025-02-04T08:13:26.434807Z","shell.execute_reply":"2025-02-04T08:13:28.119336Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"generated_ids = model.generate(model_inputs, max_new_tokens=128, do_sample=True, top_p=0.3, temperature=1.0, top_k=5)\n#generated_ids = model.generate(model_inputs, max_new_tokens=128, do_sample=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:13:39.658873Z","iopub.execute_input":"2025-02-04T08:13:39.659251Z","iopub.status.idle":"2025-02-04T08:13:44.310082Z","shell.execute_reply.started":"2025-02-04T08:13:39.659223Z","shell.execute_reply":"2025-02-04T08:13:44.309272Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"decoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T08:13:52.669473Z","iopub.execute_input":"2025-02-04T08:13:52.669768Z","iopub.status.idle":"2025-02-04T08:13:52.675102Z","shell.execute_reply.started":"2025-02-04T08:13:52.669740Z","shell.execute_reply":"2025-02-04T08:13:52.674256Z"}},"outputs":[{"name":"stdout","text":"<|user|>\nHow do chat templates work?</s> \n<|assistant|>\nChat templates help  LLMs like me generate more coherent responses by providing a structured way to organize the conversation.</s> \n<|user|>\nHow do I use them?</s> \n<|assistant|>\nTo use chat templates, you can follow these steps:\n\n1. Log in to your LLM chatbot platform.\n2. Click on the \"Templates\" tab at the top of the page.\n3. Click on the \"Add Template\" button.\n4. Choose the template you want to use from the list of available templates.\n5. Customize the template by adding your own text, images, and other elements.\n6. Save your template and use it in your chatbot conversations.\n\nOnce you have created a chat template, you can use it to generate more coherent\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}